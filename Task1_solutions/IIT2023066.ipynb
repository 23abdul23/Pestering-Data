{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1CC83w_dpBeP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Step 1: Download the latest version of the dataset\n",
        "path = kagglehub.dataset_download(\"saumyaagrawal1709/opencode-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Step 2: Define paths for the downloaded dataset and reorganized dataset\n",
        "# Adjusting to match the observed structure\n",
        "source_root = Path(path) / \"Pest and Disease/Dataset\"  # Adjust this dynamically if needed\n",
        "organized_root = Path(\"./Dataset\")\n",
        "train_dir = organized_root / \"train\"\n",
        "test_dir = organized_root / \"test\"\n",
        "\n",
        "# Create the directories for train and test if they don't exist\n",
        "train_dir.mkdir(parents=True, exist_ok=True)\n",
        "test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check if source_root exists\n",
        "if not source_root.exists():\n",
        "    print(\"Contents of the dataset directory:\")\n",
        "    for root, dirs, files in os.walk(Path(path)):\n",
        "        print(f\"Directory: {root}\")\n",
        "        print(f\"Subdirectories: {dirs}\")\n",
        "        print(f\"Files: {files}\")\n",
        "        print(\"-\" * 40)\n",
        "    raise FileNotFoundError(f\"Source directory not found: {source_root}\")\n",
        "\n",
        "# Step 3: Reorganize the dataset\n",
        "def reorganize_dataset(source_root, train_dir, test_dir):\n",
        "    for crop in source_root.iterdir():\n",
        "        if crop.is_dir():  # Crop directories: Cashew, Cassava, Maize, Tomato\n",
        "            print(f\"Processing crop: {crop.name}\")\n",
        "            for subset in [\"train_set\", \"test_set\"]:\n",
        "                subset_dir = crop / subset\n",
        "                if subset_dir.exists():\n",
        "                    print(f\"  Found subset: {subset}\")\n",
        "                    for class_dir in subset_dir.iterdir():\n",
        "                        if class_dir.is_dir():  # Class folders (e.g., anthracnose, healthy)\n",
        "                            new_class_name = f\"{class_dir.name}_{crop.name.lower()}\"\n",
        "                            target_subdir = train_dir if subset == \"train_set\" else test_dir\n",
        "                            target_class_dir = target_subdir / new_class_name\n",
        "                            target_class_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                            for file in class_dir.iterdir():\n",
        "                                if file.is_file():\n",
        "                                    try:\n",
        "                                        shutil.move(str(file), str(target_class_dir))\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Error moving file {file}: {e}\")\n",
        "    print(\"Reorganization complete. Verifying files...\")\n",
        "\n",
        "    # Verify dataset structure\n",
        "    for subset, subset_dir in [(\"train\", train_dir), (\"test\", test_dir)]:\n",
        "        print(f\"{subset.capitalize()} set:\")\n",
        "        for class_dir in subset_dir.iterdir():\n",
        "            if class_dir.is_dir():\n",
        "                print(f\"  {class_dir.name}: {len(list(class_dir.iterdir()))} files\")\n",
        "\n",
        "# Run the reorganization\n",
        "reorganize_dataset(source_root, train_dir, test_dir)\n",
        "print(\"Dataset reorganized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcljODJ8pNRP",
        "outputId": "75688ea0-adae-4fdb-ee28-ecaa03e92883"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/saumyaagrawal1709/opencode-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.26G/6.26G [00:43<00:00, 154MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/saumyaagrawal1709/opencode-dataset/versions/2\n",
            "Processing crop: Cashew\n",
            "  Found subset: train_set\n",
            "  Found subset: test_set\n",
            "Processing crop: Tomato\n",
            "  Found subset: train_set\n",
            "  Found subset: test_set\n",
            "Processing crop: Cassava\n",
            "  Found subset: train_set\n",
            "  Found subset: test_set\n",
            "Processing crop: Maize\n",
            "  Found subset: train_set\n",
            "  Found subset: test_set\n",
            "Reorganization complete. Verifying files...\n",
            "Train set:\n",
            "  mosaic_cassava: 2250 files\n",
            "  brown spot_cassava: 3250 files\n",
            "  healthy_tomato: 2000 files\n",
            "  fall armyworm_maize: 1140 files\n",
            "  bacterial blight_cassava: 3241 files\n",
            "  healthy_cashew: 5877 files\n",
            "  septoria leaf spot_tomato: 9373 files\n",
            "  red rust_cashew: 4751 files\n",
            "  verticulium wilt_tomato: 3100 files\n",
            "  green mite_cassava: 3246 files\n",
            "  leaf curl_tomato: 2050 files\n",
            "  gumosis_cashew: 1714 files\n",
            "  leaf spot_maize: 3024 files\n",
            "  grasshoper_maize: 2575 files\n",
            "  leaf blight_maize: 4025 files\n",
            "  leaf miner_cashew: 3466 files\n",
            "  streak virus_maize: 4043 files\n",
            "  anthracnose_cashew: 3102 files\n",
            "  healthy_maize: 830 files\n",
            "  healthy_cassava: 2271 files\n",
            "  leaf blight_tomato: 5200 files\n",
            "  leaf beetle_maize: 3789 files\n",
            "Test set:\n",
            "  mosaic_cassava: 1200 files\n",
            "  brown spot_cassava: 1483 files\n",
            "  healthy_tomato: 500 files\n",
            "  fall armyworm_maize: 284 files\n",
            "  bacterial blight_cassava: 2623 files\n",
            "  healthy_cashew: 1336 files\n",
            "  septoria leaf spot_tomato: 2340 files\n",
            "  red rust_cashew: 1815 files\n",
            "  verticulium wilt_tomato: 764 files\n",
            "  green mite_cassava: 1020 files\n",
            "  leaf curl_tomato: 532 files\n",
            "  gumosis_cashew: 425 files\n",
            "  leaf spot_maize: 1261 files\n",
            "  grasshoper_maize: 411 files\n",
            "  leaf blight_maize: 1004 files\n",
            "  leaf miner_cashew: 1487 files\n",
            "  streak virus_maize: 1004 files\n",
            "  anthracnose_cashew: 1838 files\n",
            "  healthy_maize: 211 files\n",
            "  healthy_cassava: 1184 files\n",
            "  leaf blight_tomato: 1309 files\n",
            "  leaf beetle_maize: 950 files\n",
            "Dataset reorganized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageDataGenerator(rescale=1.0/255)\n",
        "test_data = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "\n",
        "train_generator = train_data.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_data.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "print(\"Classes found in the dataset:\", train_generator.class_indices)\n",
        "\n",
        "# Do not change this code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0RwGqEQpRJK",
        "outputId": "f245f56a-638e-4ab9-de3f-fd736840fc16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 74317 images belonging to 22 classes.\n",
            "Found 24981 images belonging to 22 classes.\n",
            "Classes found in the dataset: {'anthracnose_cashew': 0, 'bacterial blight_cassava': 1, 'brown spot_cassava': 2, 'fall armyworm_maize': 3, 'grasshoper_maize': 4, 'green mite_cassava': 5, 'gumosis_cashew': 6, 'healthy_cashew': 7, 'healthy_cassava': 8, 'healthy_maize': 9, 'healthy_tomato': 10, 'leaf beetle_maize': 11, 'leaf blight_maize': 12, 'leaf blight_tomato': 13, 'leaf curl_tomato': 14, 'leaf miner_cashew': 15, 'leaf spot_maize': 16, 'mosaic_cassava': 17, 'red rust_cashew': 18, 'septoria leaf spot_tomato': 19, 'streak virus_maize': 20, 'verticulium wilt_tomato': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPfmkChNpv5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}